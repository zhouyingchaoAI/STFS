"""
è‡ªç„¶è¯­è¨€æŸ¥è¯¢API
"""
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
import time
import json
import asyncio
from datetime import datetime, date
from decimal import Decimal

from app.models.query import QueryRequest, QueryResponse
from app.config import settings
from app.core.nl2sql.hybrid_engine import HybridNL2SQLEngine
from app.core.query_executor.query_executor import QueryExecutor
from app.core.result_processor.formatter import ResultFormatter
from app.core.result_processor.chart_generator import ChartGenerator
from app.utils.logger import setup_logger

router = APIRouter()
logger = setup_logger(__name__)


# åˆå§‹åŒ–ç»„ä»¶ï¼ˆä½¿ç”¨æ··åˆå¼•æ“ï¼‰
nl2sql_engine = HybridNL2SQLEngine()
query_executor = QueryExecutor()
result_formatter = ResultFormatter()
chart_generator = ChartGenerator()


@router.post("/query", response_model=QueryResponse)
async def natural_language_query(request: QueryRequest):
    """
    è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¥å£
    
    å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSQLå¹¶æ‰§è¡ŒæŸ¥è¯¢
    """
    start_time = time.time()
    process_steps = []  # è®°å½•å¤„ç†è¿‡ç¨‹
    
    # æ‰“å°å¯¹è¯æé—®ä¿¡æ¯
    logger.info("=" * 80)
    logger.info("ğŸ“ æ”¶åˆ°å¯¹è¯æé—®")
    logger.info(f"   é—®é¢˜: {request.question}")
    logger.info(f"   æŸ¥è¯¢é€‰é¡¹: {request.options}")
    if request.conversation_history:
        logger.info(f"   å¯¹è¯å†å² (å…±{len(request.conversation_history)}æ¡):")
        for i, hist in enumerate(request.conversation_history, 1):
            logger.info(f"     [{i}] é—®é¢˜: {hist.get('question', 'N/A')}")
            if hist.get('error'):
                logger.info(f"           é”™è¯¯: {hist.get('error', 'N/A')}")
            if hist.get('sql'):
                logger.info(f"           SQL: {hist.get('sql', 'N/A')[:100]}...")
    else:
        logger.info("   å¯¹è¯å†å²: æ— ")
    logger.info("=" * 80)
    
    try:
        # 1. NL2SQLè½¬æ¢
        logger.info(f"Processing query: {request.question}")
        query_options = request.options or {}
        use_llm = query_options.get("use_llm", False)  # æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨LLM
        
        process_steps.append({
            "step": "ç†è§£é—®é¢˜",
            "status": "processing",
            "message": "æ­£åœ¨åˆ†æè‡ªç„¶è¯­è¨€é—®é¢˜...",
            "timestamp": time.time()
        })
        
        nl2sql_start = time.time()
        # è·å–å¯¹è¯å†å²ï¼ˆç”¨äºå¤šè½®å¯¹è¯ä¿®æ­£ï¼‰
        conversation_history = request.conversation_history if request.conversation_history is not None else []
        sql_result = nl2sql_engine.convert(request.question, use_llm=use_llm, conversation_history=conversation_history)
        nl2sql_time = time.time() - nl2sql_start
        
        if not sql_result or not sql_result.get("sql"):
            process_steps.append({
                "step": "ç†è§£é—®é¢˜",
                "status": "error",
                "message": "æ— æ³•ç†è§£æŸ¥è¯¢æ„å›¾",
                "timestamp": time.time()
            })
            raise HTTPException(
                status_code=400,
                detail={
                    "code": 400,
                    "message": "æ— æ³•ç†è§£æŸ¥è¯¢æ„å›¾ï¼Œè¯·å°è¯•é‡æ–°è¡¨è¿°é—®é¢˜",
                    "data": None
                }
            )
        
        sql_query = sql_result["sql"]
        intent = sql_result.get("intent")
        entities = sql_result.get("entities", {})
        engine_type = sql_result.get("engine_type", "rule")  # rule æˆ– llm
        thinking_process = sql_result.get("thinking_process", "")
        
        process_steps.append({
            "step": "ç†è§£é—®é¢˜",
            "status": "success",
            "message": f"ä½¿ç”¨{engine_type}å¼•æ“å®Œæˆæ„å›¾è¯†åˆ«",
            "details": {
                "intent": intent,
                "entities": entities,
                "thinking": thinking_process if thinking_process else f"ä½¿ç”¨{engine_type}å¼•æ“åˆ†æé—®é¢˜ï¼š\n1. è¯†åˆ«æŸ¥è¯¢æ„å›¾ä¸ºï¼š{intent}\n2. æå–å®ä½“ä¿¡æ¯ï¼š{entities}\n3. åŒ¹é…æŸ¥è¯¢æ¨¡æ¿å¹¶ç”ŸæˆSQL"
            },
            "duration": round(nl2sql_time, 3),
            "timestamp": time.time()
        })
        
        logger.info(f"Generated SQL: {sql_query}")
        logger.info(f"Intent: {intent}, Entities: {entities}")
        
        # 2. ç”ŸæˆSQL
        process_steps.append({
            "step": "ç”ŸæˆSQL",
            "status": "processing",
            "message": "æ­£åœ¨ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥...",
            "timestamp": time.time()
        })
        
        process_steps.append({
            "step": "ç”ŸæˆSQL",
            "status": "success",
            "message": "SQLè¯­å¥ç”Ÿæˆå®Œæˆ",
            "details": {
                "sql": sql_query
            },
            "timestamp": time.time()
        })
        
        # 3. æ‰§è¡ŒæŸ¥è¯¢
        process_steps.append({
            "step": "æ‰§è¡ŒæŸ¥è¯¢",
            "status": "processing",
            "message": "æ­£åœ¨æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢...",
            "timestamp": time.time()
        })
        
        database = query_options.get("database", "master")
        max_rows = query_options.get("max_rows", 10000)
        
        query_start = time.time()
        query_result = query_executor.execute(
            sql=sql_query,
            database=database,
            max_rows=max_rows
        )
        query_time = time.time() - query_start
        
        process_steps.append({
            "step": "æ‰§è¡ŒæŸ¥è¯¢",
            "status": "success",
            "message": f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œè¿”å› {query_result.get('row_count', 0)} è¡Œæ•°æ®",
            "details": {
                "row_count": query_result.get("row_count", 0),
                "database": database
            },
            "duration": round(query_time, 3),
            "timestamp": time.time()
        })
        
        # 4. æ ¼å¼åŒ–ç»“æœ
        process_steps.append({
            "step": "å¤„ç†ç»“æœ",
            "status": "processing",
            "message": "æ­£åœ¨æ ¼å¼åŒ–æŸ¥è¯¢ç»“æœ...",
            "timestamp": time.time()
        })
        
        format_start = time.time()
        formatted_result = result_formatter.format(query_result)
        format_time = time.time() - format_start
        
        # 5. ç”Ÿæˆå›¾è¡¨é…ç½®
        chart_start = time.time()
        chart_config = chart_generator.generate(
            data=formatted_result,
            intent=intent
        )
        chart_time = time.time() - chart_start
        
        process_steps.append({
            "step": "å¤„ç†ç»“æœ",
            "status": "success",
            "message": "ç»“æœæ ¼å¼åŒ–å’Œå›¾è¡¨é…ç½®å®Œæˆ",
            "details": {
                "has_chart": chart_config is not None
            },
            "duration": round(format_time + chart_time, 3),
            "timestamp": time.time()
        })
        
        execution_time = time.time() - start_time
        
        # 6. æ„å»ºå“åº”
        response_data = {
            "sql": sql_query,
            "result": formatted_result,
            "chart_config": chart_config,
            "execution_time": round(execution_time, 3),
            "row_count": len(formatted_result),
            "process_steps": process_steps  # æ·»åŠ è¿‡ç¨‹ä¿¡æ¯
        }
        
        return QueryResponse(
            code=200,
            message="success",
            data=response_data,
            metadata={
                "intent": intent,
                "entities": entities,
                "engine_type": engine_type,
                "thinking_process": thinking_process
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Query execution error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "code": 500,
                "message": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}",
                "data": None
            }
        )


@router.post("/query/stream")
async def natural_language_query_stream(request: QueryRequest):
    """
    è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ¥å£ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
    
    ä½¿ç”¨Server-Sent Events (SSE)å®æ—¶è¿”å›æ€è€ƒè¿‡ç¨‹å’ŒæŸ¥è¯¢ç»“æœ
    """
    # æ‰“å°å¯¹è¯æé—®ä¿¡æ¯
    logger.info("=" * 80)
    logger.info("ğŸ“ æ”¶åˆ°å¯¹è¯æé—® (æµå¼)")
    logger.info(f"   é—®é¢˜: {request.question}")
    logger.info(f"   æŸ¥è¯¢é€‰é¡¹: {request.options}")
    if request.conversation_history:
        logger.info(f"   å¯¹è¯å†å² (å…±{len(request.conversation_history)}æ¡):")
        for i, hist in enumerate(request.conversation_history, 1):
            logger.info(f"     [{i}] é—®é¢˜: {hist.get('question', 'N/A')}")
            if hist.get('error'):
                logger.info(f"           é”™è¯¯: {hist.get('error', 'N/A')}")
            if hist.get('sql'):
                logger.info(f"           SQL: {hist.get('sql', 'N/A')[:100]}...")
    else:
        logger.info("   å¯¹è¯å†å²: æ— ")
    logger.info("=" * 80)
    
    async def generate():
        start_time = time.time()
        process_steps = []
        
        try:
            query_options = request.options or {}
            use_llm = query_options.get("use_llm", False)
            
            # å‘é€å¼€å§‹æ€è€ƒçš„ä¿¡å·
            yield f"data: {json.dumps({'type': 'thinking_start', 'message': 'å¼€å§‹åˆ†æé—®é¢˜...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            # 1. NL2SQLè½¬æ¢ï¼ˆæµå¼ï¼‰
            if use_llm:
                # ä½¿ç”¨LLMå¼•æ“çš„æµå¼è°ƒç”¨
                from app.core.nl2sql.llm_based_engine import LLMBasedNL2SQLEngine
                llm_engine = LLMBasedNL2SQLEngine()
                
                # è·å–å¯¹è¯å†å²ï¼ˆç”¨äºå¤šè½®å¯¹è¯ä¿®æ­£ï¼‰
                conversation_history = request.conversation_history if request.conversation_history is not None else []
                
                # æµå¼è°ƒç”¨LLM
                full_response = ""
                
                prompt = llm_engine._build_prompt(request.question, conversation_history)
                logger.info(f"å¼€å§‹æµå¼è°ƒç”¨LLMï¼Œprompté•¿åº¦: {len(prompt)}")
                
                try:
                    response_stream = llm_engine._call_ollama_stream(prompt)
                    logger.info("Ollamaæµå¼å“åº”å·²å¯åŠ¨")
                except Exception as stream_error:
                    logger.error(f"å¯åŠ¨Ollamaæµå¼å“åº”å¤±è´¥: {stream_error}", exc_info=True)
                    # å‘é€é”™è¯¯ä¿¡æ¯
                    error_msg = f"æ— æ³•è¿æ¥åˆ°LLMæœåŠ¡: {str(stream_error)}"
                    error_content = f"âŒ {error_msg}\n"
                    yield f"data: {json.dumps({'type': 'thinking', 'content': error_content}, ensure_ascii=False)}\n\n"
                    yield f"data: {json.dumps({'type': 'error', 'message': error_msg, 'details': str(stream_error)}, ensure_ascii=False)}\n\n"
                    return
                
                # ç”¨äºè·Ÿè¸ªæ˜¯å¦å·²ç»é‡åˆ°SELECT
                found_select = False
                
                chunk_count = 0
                thinking_char_count = 0
                has_received_any_chunk = False
                
                # ç”¨äºè·Ÿè¸ªæ˜¯å¦å·²ç»å‘é€äº†åˆå§‹æç¤ºï¼ˆå¦‚æœç­‰å¾…ä¸€æ®µæ—¶é—´è¿˜æ²¡æ”¶åˆ°å†…å®¹ï¼Œå†å‘é€æç¤ºï¼‰
                initial_prompt_sent = False
                first_chunk_time = None
                last_chunk_time = None
                no_chunk_timeout = 5.0  # 5ç§’æ²¡æœ‰æ”¶åˆ°chunkåˆ™å‘é€æç¤º
                
                logger.info("å¼€å§‹æ¥æ”¶Ollamaæµå¼å“åº”chunks...")
                
                for chunk in response_stream:
                    chunk_count += 1
                    current_time = asyncio.get_event_loop().time()
                    
                    # æ£€æŸ¥æ˜¯å¦é•¿æ—¶é—´æ²¡æœ‰æ”¶åˆ°chunk
                    if last_chunk_time is not None and (current_time - last_chunk_time) > no_chunk_timeout:
                        logger.warning(f"è¶…è¿‡{no_chunk_timeout}ç§’æ²¡æœ‰æ”¶åˆ°chunkï¼Œå¯èƒ½Ollamaå“åº”è¾ƒæ…¢")
                        if not has_received_any_chunk:
                            # å¦‚æœè¿˜æ²¡æ”¶åˆ°ä»»ä½•chunkï¼Œå‘é€ç­‰å¾…æç¤º
                            # f-stringä¸­ä¸èƒ½åŒ…å«åæ–œæ ï¼Œå…ˆå®šä¹‰å­—ç¬¦ä¸²
                            waiting_msg = 'â³ æ­£åœ¨ç­‰å¾…LLMå“åº”ï¼Œè¯·ç¨å€™...\n\n'
                            yield f"data: {json.dumps({'type': 'thinking', 'content': waiting_msg}, ensure_ascii=False)}\n\n"
                    
                    if chunk:
                        # Ollamaè¿”å›æ ¼å¼: {"response": "text", "done": False}
                        chunk_text = chunk.get('response', '') if isinstance(chunk, dict) else str(chunk)
                        is_done = chunk.get('done', False) if isinstance(chunk, dict) else False
                        
                        # æ›´æ–°æœ€åæ”¶åˆ°chunkçš„æ—¶é—´
                        last_chunk_time = current_time
                        
                        if chunk_text:
                            has_received_any_chunk = True
                            
                            # è®°å½•ç¬¬ä¸€ä¸ªchunkçš„æ—¶é—´
                            if first_chunk_time is None:
                                first_chunk_time = current_time
                            
                            full_response += chunk_text
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«SELECTï¼ˆè¡¨ç¤ºSQLå¼€å§‹ï¼‰
                            if 'SELECT' in chunk_text.upper() and not found_select:
                                # å¦‚æœchunkä¸­åŒ…å«SELECTï¼Œæå–SELECTä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºæ€è€ƒå†…å®¹
                                select_index = chunk_text.upper().find('SELECT')
                                if select_index > 0:
                                    thinking_part = chunk_text[:select_index]
                                    # ç«‹å³å‘é€æ€è€ƒå†…å®¹ï¼ˆæ•´ä¸ªéƒ¨åˆ†ï¼‰
                                    if thinking_part:
                                        yield f"data: {json.dumps({'type': 'thinking', 'content': thinking_part}, ensure_ascii=False)}\n\n"
                                        thinking_char_count += len(thinking_part)
                                
                                # æ ‡è®°å·²æ‰¾åˆ°SELECTï¼Œä¹‹åçš„å†…å®¹ä¸å†ä½œä¸ºæ€è€ƒå†…å®¹å‘é€
                                found_select = True
                                # ç»§ç»­æ”¶é›†SELECTåŠä¹‹åçš„å†…å®¹åˆ°full_responseï¼ˆä¸å‘é€ï¼Œä½†éœ€è¦å®Œæ•´å“åº”æ¥è§£æSQLï¼‰
                                
                            elif not found_select:
                                # æ²¡æœ‰SELECTï¼Œç«‹å³å‘é€è¿™ä¸ªchunkä½œä¸ºæ€è€ƒå†…å®¹
                                # ç›´æ¥å‘é€æ•´ä¸ªchunkï¼ˆOllamaçš„chunké€šå¸¸å·²ç»å¾ˆå°äº†ï¼Œæ¯ä¸ªchunkå¯èƒ½åªæœ‰å‡ ä¸ªå­—ç¬¦ï¼‰
                                # å³ä½¿chunkå¾ˆå°ï¼ˆå¦‚'<think>'ã€'\n'ç­‰ï¼‰ï¼Œä¹Ÿè¦å‘é€
                                if chunk_text:
                                    # å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªæœ‰å†…å®¹çš„chunkï¼Œä¸”ä¹‹å‰æ²¡æœ‰å‘é€è¿‡åˆå§‹æç¤ºï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªç®€å•çš„æç¤º
                                    if thinking_char_count == 0 and not initial_prompt_sent:
                                        # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ·»åŠ ä¿®æ­£æç¤º
                                        if conversation_history:
                                            last_error = conversation_history[-1].get("error")
                                            if last_error:
                                                # f-stringä¸­ä¸èƒ½åŒ…å«åæ–œæ ï¼Œå…ˆå®šä¹‰å­—ç¬¦ä¸²
                                                correction_msg = 'ğŸ“ æ£€æµ‹åˆ°ä¹‹å‰çš„æŸ¥è¯¢é”™è¯¯ï¼Œæ­£åœ¨æ ¹æ®é”™è¯¯ä¿¡æ¯è¿›è¡Œä¿®æ­£...\n\n'
                                                yield f"data: {json.dumps({'type': 'thinking', 'content': correction_msg}, ensure_ascii=False)}\n\n"
                                                initial_prompt_sent = True
                                    
                                    # ç«‹å³å‘é€ï¼Œä¸ç­‰å¾…ï¼Œä¸ç¼“å†²ï¼Œç¡®ä¿æ¯ä¸ªtokenéƒ½ç«‹å³æ˜¾ç¤º
                                    # ä½¿ç”¨flushç¡®ä¿ç«‹å³å‘é€ï¼ˆè™½ç„¶SSEä¼šè‡ªåŠ¨flushï¼Œä½†è¿™é‡Œæ˜ç¡®è¯´æ˜ï¼‰
                                    thinking_event = f"data: {json.dumps({'type': 'thinking', 'content': chunk_text}, ensure_ascii=False)}\n\n"
                                    yield thinking_event
                                    thinking_char_count += len(chunk_text)
                            
                            # å¦‚æœå·²ç»æ‰¾åˆ°SELECTï¼Œç»§ç»­æ”¶é›†å‰©ä½™å†…å®¹åˆ°full_responseï¼ˆä¸å‘é€ï¼Œä½†éœ€è¦å®Œæ•´å“åº”ï¼‰
                        
                        # å¦‚æœdone=Trueï¼Œè¡¨ç¤ºæµå¼å“åº”ç»“æŸ
                        if is_done:
                            # æ€è€ƒè¿‡ç¨‹å®Œæˆï¼Œç«‹å³å‘é€å®Œæˆä¿¡å·
                            if thinking_char_count > 0:
                                yield f"data: {json.dumps({'type': 'thinking_complete', 'message': 'æ€è€ƒè¿‡ç¨‹å·²å®Œæˆ'}, ensure_ascii=False)}\n\n"
                                await asyncio.sleep(0.001)
                            break
                
                # æ£€æŸ¥æ˜¯å¦æ”¶åˆ°äº†ä»»ä½•chunk
                if not has_received_any_chunk:
                    logger.warning("Ollamaæµå¼å“åº”æ²¡æœ‰è¿”å›ä»»ä½•chunkï¼")
                    warning_content = "âš ï¸ LLMæ²¡æœ‰è¿”å›ä»»ä½•å“åº”ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ\n"
                    yield f"data: {json.dumps({'type': 'thinking', 'content': warning_content}, ensure_ascii=False)}\n\n"
                    yield f"data: {json.dumps({'type': 'error', 'message': 'LLMæ²¡æœ‰è¿”å›å“åº”', 'details': 'Ollamaæµå¼å“åº”ä¸ºç©º'}, ensure_ascii=False)}\n\n"
                    return
                
                # å¦‚æœæ”¶åˆ°äº†chunkä½†æ²¡æœ‰å‘é€ä»»ä½•æ€è€ƒå†…å®¹ï¼ˆå¯èƒ½ç›´æ¥è¿”å›äº†SQLï¼‰ï¼Œå‘é€ä¸€ä¸ªæç¤º
                if has_received_any_chunk and thinking_char_count == 0 and not found_select:
                    # æ£€æŸ¥full_responseæ˜¯å¦ç›´æ¥æ˜¯SQLï¼ˆæ²¡æœ‰æ€è€ƒè¿‡ç¨‹ï¼‰
                    if 'SELECT' in full_response.upper():
                        # ç›´æ¥è¿”å›äº†SQLï¼Œæ²¡æœ‰æ€è€ƒè¿‡ç¨‹ï¼Œå‘é€ä¸€ä¸ªç®€çŸ­æç¤º
                        info_msg = "ğŸ’­ æ­£åœ¨åˆ†æé—®é¢˜å¹¶ç”ŸæˆSQLæŸ¥è¯¢...\n\n"
                        yield f"data: {json.dumps({'type': 'thinking', 'content': info_msg}, ensure_ascii=False)}\n\n"
                        thinking_char_count += len(info_msg)
                
                # è®°å½•å®Œæ•´å“åº”ç”¨äºè°ƒè¯•
                # æµå¼å¤„ç†å®Œæˆ
                has_select = 'SELECT' in full_response.upper()
                resp_preview = full_response[:200] if len(full_response) > 200 else full_response
                logger.info(f"LLMå®Œæ•´å“åº”é•¿åº¦: {len(full_response)}, åŒ…å«SELECT: {has_select}")
                logger.info(f"LLMå“åº”å‰200å­—ç¬¦: {resp_preview}")
                if not has_select:
                    logger.warning(f"LLMå“åº”ä¸­æœªæ‰¾åˆ°SELECTè¯­å¥ï¼Œå®Œæ•´å“åº”: {full_response[:500]}")
                
                # è§£æå®Œæ•´å“åº”
                try:
                    sql_result = llm_engine._parse_response(full_response, request.question)
                except Exception as parse_error:
                    logger.error(f"LLMå“åº”è§£æå¤±è´¥: {parse_error}")
                    logger.error(f"å®Œæ•´å“åº”å†…å®¹: {full_response[:1000]}")
                    # å‘é€è§£æé”™è¯¯ä¿¡æ¯
                    parse_error_msg = f"\n\n[LLMå“åº”è§£æå¤±è´¥: {str(parse_error)}]"
                    yield f"data: {json.dumps({'type': 'thinking', 'content': parse_error_msg}, ensure_ascii=False)}\n\n"
                    sql_result = None
                
                # å¦‚æœLLMè§£æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™å¼•æ“
                if not sql_result or not sql_result.get("sql"):
                    logger.warning("LLMè§£æå¤±è´¥ï¼Œå›é€€åˆ°è§„åˆ™å¼•æ“")
                    resp_preview = full_response[:500] if len(full_response) > 500 else full_response
                    logger.warning(f"å®Œæ•´å“åº”å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰: {resp_preview}")
                    # å‘é€å¤±è´¥ä¿¡æ¯å’Œå®Œæ•´å“åº”ç”¨äºè°ƒè¯•
                    fallback_msg = f"\n\n[LLMè§£æå¤±è´¥ï¼Œåˆ‡æ¢åˆ°è§„åˆ™å¼•æ“...]\n[å·²æ”¶é›†çš„LLMå“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰:\n{resp_preview}]"
                    yield f"data: {json.dumps({'type': 'thinking', 'content': fallback_msg}, ensure_ascii=False)}\n\n"
                    conversation_history = request.conversation_history if request.conversation_history is not None else []
                    sql_result = nl2sql_engine.convert(request.question, use_llm=False, conversation_history=conversation_history)
            else:
                # ä½¿ç”¨è§„åˆ™å¼•æ“ï¼ˆéæµå¼ï¼‰
                yield f"data: {json.dumps({'type': 'thinking', 'content': 'æ­£åœ¨ä½¿ç”¨è§„åˆ™å¼•æ“åˆ†æé—®é¢˜...'}, ensure_ascii=False)}\n\n"
                sql_result = nl2sql_engine.convert(request.question, use_llm=False)
            
            if not sql_result or not sql_result.get("sql"):
                yield f"data: {json.dumps({'type': 'error', 'message': 'æ— æ³•ç†è§£æŸ¥è¯¢æ„å›¾'}, ensure_ascii=False)}\n\n"
                return
            
            sql_query = sql_result["sql"]
            thinking_process = sql_result.get("thinking_process", "")
            
            # 1. ç«‹å³å‘é€SQLç”Ÿæˆå®Œæˆï¼ˆé˜¶æ®µ1å®Œæˆï¼‰
            yield f"data: {json.dumps({'type': 'sql_generated', 'sql': sql_query, 'thinking': thinking_process}, ensure_ascii=False)}\n\n"
            logger.info("[é˜¶æ®µ1å®Œæˆ] SQLç”Ÿæˆå®Œæˆï¼Œç«‹å³å‘é€åˆ°å‰ç«¯")
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            # 2. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆé˜¶æ®µ2ï¼‰- å¤šè½®å¯¹è¯æ¨¡å¼ï¼šå¤±è´¥æ—¶è®°å½•é”™è¯¯ï¼Œä¸è‡ªåŠ¨é‡è¯•
            database = query_options.get("database", "master")
            max_rows = query_options.get("max_rows", 10000)
            
            yield f"data: {json.dumps({'type': 'step', 'step': 'æ‰§è¡ŒæŸ¥è¯¢', 'status': 'processing', 'message': 'æ­£åœ¨æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            try:
                query_result = query_executor.execute(
                    sql=sql_query,
                    database=database,
                    max_rows=max_rows
                )
            except Exception as db_error:
                # æŸ¥è¯¢å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯åˆ°å¯¹è¯å†å²ï¼Œä¸è‡ªåŠ¨é‡è¯•
                error_msg = str(db_error)
                logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {error_msg}")
                logger.error(f"å¤±è´¥çš„SQL: {sql_query}")
                
                # ç¡®ä¿æ€è€ƒè¿‡ç¨‹å·²ç»å®Œæ•´å‘é€ï¼ˆå¦‚æœè¿˜æ²¡æœ‰å‘é€å®Œæˆä¿¡å·ï¼Œç°åœ¨å‘é€ï¼‰
                if thinking_process:
                    # å‘é€æ€è€ƒè¿‡ç¨‹å®Œæˆä¿¡å·ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰å‘é€ï¼‰
                    yield f"data: {json.dumps({'type': 'thinking_complete', 'message': 'æ€è€ƒè¿‡ç¨‹å·²å®Œæˆ'}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.001)
                
                # å‘é€é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯ï¼ˆåŒ…å«é”™è¯¯è¯¦æƒ…å’Œå¤±è´¥çš„SQLï¼Œä¾›ä¸‹ä¸€è½®å¯¹è¯ä½¿ç”¨ï¼‰
                yield f"data: {json.dumps({'type': 'step', 'step': 'æ‰§è¡ŒæŸ¥è¯¢', 'status': 'error', 'message': f'æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {error_msg}'}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.001)
                
                # å‘é€è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«å¤±è´¥çš„SQLå’Œæ€è€ƒè¿‡ç¨‹ï¼Œä¾›ä¸‹ä¸€è½®å¯¹è¯ä¿®æ­£ä½¿ç”¨
                error_details = {
                    "error": error_msg,
                    "failed_sql": sql_query,
                    "original_question": request.question,
                    "thinking_process": thinking_process,  # åŒ…å«æ€è€ƒè¿‡ç¨‹
                    "suggestion": "æ‚¨å¯ä»¥åœ¨ä¸‹ä¸€è½®å¯¹è¯ä¸­æä¾›æ›´å¤šä¿¡æ¯ï¼Œæˆ–ç›´æ¥è¯´'ä¿®æ­£SQL'ã€'é‡æ–°æŸ¥è¯¢'ç­‰ï¼Œç³»ç»Ÿä¼šæ ¹æ®é”™è¯¯ä¿¡æ¯è‡ªåŠ¨ä¿®æ­£ã€‚"
                }
                yield f"data: {json.dumps({'type': 'error', 'message': f'æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {error_msg}', 'details': error_details, 'sql': sql_query, 'thinking': thinking_process, 'can_retry': True}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.001)
                return
            
            # æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ
            row_count = query_result.get("row_count", 0)
            query_time = time.time() - start_time
            
            # 2.1 ç«‹å³å‘é€æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼ˆé˜¶æ®µ2å®Œæˆï¼‰
            success_msg = f'æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œè¿”å› {row_count} è¡Œæ•°æ®'
            yield f"data: {json.dumps({'type': 'step', 'step': 'æ‰§è¡ŒæŸ¥è¯¢', 'status': 'success', 'message': success_msg, 'row_count': row_count}, ensure_ascii=False)}\n\n"
            logger.info(f"[é˜¶æ®µ2å®Œæˆ] æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œè¿”å› {row_count} è¡Œæ•°æ®ï¼Œç«‹å³å‘é€åˆ°å‰ç«¯")
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            # 2.2 ç«‹å³å‘é€æŸ¥è¯¢ç»“æœï¼ˆéƒ¨åˆ†æ•°æ®ï¼‰- æ‰§è¡Œå®Œæˆåç«‹å³æ˜¾ç¤º
            if query_result.get("data"):
                # å‘é€å‰å‡ è¡Œæ•°æ®ï¼Œè®©ç”¨æˆ·å…ˆçœ‹åˆ°ç»“æœ
                preview_data = query_result["data"][:10]  # å‰10è¡Œ
                # éœ€è¦å¤„ç†Decimalåºåˆ—åŒ–
                def preview_serializer(obj):
                    if isinstance(obj, (datetime, date)):
                        return obj.isoformat()
                    elif isinstance(obj, Decimal):
                        if obj % 1 == 0:
                            return int(obj)
                        else:
                            return float(obj)
                    return obj
                
                # åºåˆ—åŒ–é¢„è§ˆæ•°æ®
                import json as json_lib
                preview_data_serialized = json_lib.loads(json_lib.dumps(preview_data, default=preview_serializer))
                yield f"data: {json.dumps({'type': 'result_preview', 'data': preview_data_serialized, 'total_rows': row_count, 'preview_count': len(preview_data)}, ensure_ascii=False)}\n\n"
                logger.info(f"[é˜¶æ®µ2éƒ¨åˆ†] ç«‹å³å‘é€å‰10è¡Œé¢„è§ˆæ•°æ®åˆ°å‰ç«¯")
                await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            # 3. æ ¼å¼åŒ–ç»“æœï¼ˆé˜¶æ®µ3ï¼‰- ç«‹å³å¼€å§‹å¤„ç†
            yield f"data: {json.dumps({'type': 'step', 'step': 'å¤„ç†ç»“æœ', 'status': 'processing', 'message': 'æ­£åœ¨æ ¼å¼åŒ–æŸ¥è¯¢ç»“æœ...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            format_start = time.time()
            formatted_result = result_formatter.format(query_result)
            format_time = time.time() - format_start
            
            # 3.1 ç«‹å³å‘é€æ ¼å¼åŒ–å®Œæˆ
            yield f"data: {json.dumps({'type': 'step', 'step': 'å¤„ç†ç»“æœ', 'status': 'success', 'message': f'ç»“æœæ ¼å¼åŒ–å®Œæˆï¼Œå…± {row_count} è¡Œ', 'duration': round(format_time, 3)}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)
            
            # 3.1 ç«‹å³å‘é€æ ¼å¼åŒ–ç»“æœå®Œæˆï¼ˆé˜¶æ®µ3å®Œæˆï¼‰
            # éœ€è¦å¤„ç†Decimalåºåˆ—åŒ–
            def format_serializer(obj):
                if isinstance(obj, (datetime, date)):
                    return obj.isoformat()
                elif isinstance(obj, Decimal):
                    if obj % 1 == 0:
                        return int(obj)
                    else:
                        return float(obj)
                return obj
            
            formatted_result_serialized = json.loads(json.dumps(formatted_result, default=format_serializer))
            yield f"data: {json.dumps({'type': 'result_formatted', 'data': formatted_result_serialized, 'row_count': len(formatted_result)}, ensure_ascii=False)}\n\n"
            logger.info(f"[é˜¶æ®µ3å®Œæˆ] ç»“æœæ ¼å¼åŒ–å®Œæˆï¼Œç«‹å³å‘é€åˆ°å‰ç«¯")
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            # 4. ç”Ÿæˆå›¾è¡¨é…ç½®ï¼ˆé˜¶æ®µ4ï¼‰
            yield f"data: {json.dumps({'type': 'step', 'step': 'ç”Ÿæˆå›¾è¡¨', 'status': 'processing', 'message': 'æ­£åœ¨ç”Ÿæˆå›¾è¡¨é…ç½®...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            chart_config = chart_generator.generate(
                data=formatted_result,
                intent=sql_result.get("intent")
            )
            
            # 4.1 ç«‹å³å‘é€å›¾è¡¨é…ç½®å®Œæˆï¼ˆé˜¶æ®µ4å®Œæˆï¼‰
            yield f"data: {json.dumps({'type': 'chart_generated', 'chart_config': chart_config}, ensure_ascii=False)}\n\n"
            logger.info(f"[é˜¶æ®µ4å®Œæˆ] å›¾è¡¨é…ç½®ç”Ÿæˆå®Œæˆï¼Œç«‹å³å‘é€åˆ°å‰ç«¯")
            await asyncio.sleep(0.001)  # ç¡®ä¿ç«‹å³å‘é€
            
            execution_time = time.time() - start_time
            
            # 5. å‘é€æœ€ç»ˆå®Œæˆä¿¡å·ï¼ˆéœ€è¦å¤„ç†datetimeå’ŒDecimalåºåˆ—åŒ–ï¼‰
            def json_serializer(obj):
                """è‡ªå®šä¹‰JSONåºåˆ—åŒ–å™¨ï¼Œå¤„ç†datetimeå’ŒDecimalå¯¹è±¡"""
                if isinstance(obj, (datetime, date)):
                    return obj.isoformat()
                elif isinstance(obj, Decimal):
                    # Decimalè½¬æ¢ä¸ºfloatæˆ–int
                    if obj % 1 == 0:
                        return int(obj)
                    else:
                        return float(obj)
                raise TypeError(f"Type {type(obj)} not serializable")
            
            final_data = {
                "type": "complete",
                "data": {
                    "sql": sql_query,
                    "result": formatted_result,
                    "chart_config": chart_config,
                    "execution_time": round(execution_time, 3),
                    "row_count": len(formatted_result)
                },
                "metadata": {
                    "intent": sql_result.get("intent"),
                    "entities": sql_result.get("entities", {}),
                    "engine_type": "llm" if use_llm else "rule",
                    "thinking_process": thinking_process
                }
            }
            
            yield f"data: {json.dumps(final_data, default=json_serializer, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"Stream query error: {e}", exc_info=True)
            
            # å³ä½¿å¤±è´¥ï¼Œä¹Ÿè¦å‘é€å·²æ”¶é›†çš„æ€ç»´è¿‡ç¨‹å’Œå®Œæ•´å“åº”
            if 'full_response' in locals() and full_response:
                logger.info(f"å‘é€å¤±è´¥æ—¶çš„å®Œæ•´å“åº”ç”¨äºè°ƒè¯•ï¼Œé•¿åº¦: {len(full_response)}")
                resp_preview = full_response[:500] if len(full_response) > 500 else full_response
                error_info = f"\n\n[âŒ é”™è¯¯å‘ç”Ÿ]\nå·²æ”¶é›†çš„å“åº”å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰ï¼š\n{resp_preview}..."
                yield f"data: {json.dumps({'type': 'thinking', 'content': error_info}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.001)
            
            # ç«‹å³å‘é€é”™è¯¯ä¿¡æ¯åˆ°å¯¹è¯ä¸­
            error_msg = f'æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}'
            yield f"data: {json.dumps({'type': 'error', 'message': error_msg, 'details': str(e)}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)
            
            # å‘é€é”™è¯¯æ­¥éª¤
            yield f"data: {json.dumps({'type': 'step', 'step': 'æ‰§è¡Œå¤±è´¥', 'status': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.001)
            
            # å‘é€å®Œæ•´çš„é”™è¯¯å †æ ˆç”¨äºè°ƒè¯•
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"å®Œæ•´é”™è¯¯å †æ ˆ:\n{error_trace}")
            yield f"data: {json.dumps({'type': 'error_detail', 'traceback': error_trace}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

